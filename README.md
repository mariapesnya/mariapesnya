## 1) Сортировка выбором (Selection Sort) — это простой алгоритм сортировки, который на каждом шаге ищет минимальный элемент в неотсортированной части массива и меняет его местами с первым элементом этой части. Этот процесс повторяется подряд, пока весь массив не станет отсортирован.
*  **Объяснение работы скрипта:**
   * Перебираем каждый элемент массива, за исключением последнего.
   * Внутри этого цикла ищем индекс минимального элемента в оставшейся части массива.
   * Меняем текущий элемент с найденным минимальным.
   * Повторяем процесс для следующего элемента, постепенно "осветляя" массив слева направо.
*  **Временная сложность:**
   * В худшем, среднем и лучшем случаях: O(n²)
   * Алгоритм не особо эффективен для больших данных, так как количество сравнений квадратично растет с ростом массива.
*  **Объяснение:** Для каждого из n элементов нужно найти минимальный среди оставшихся, что требует n, n-1, ..., сравнений.
Общее число сравнений — сумма арифметической прогрессии, которая пропорциональна n*(n-1)/2, что асимптотически равно O(n²).

## 2) Сортивка обменом- это простой алгоритм сортировки. Он многократно проходит по массиву, сравнивает соседние элементы и меняет их местами, если они идут в неправильном порядке. Этот процесс повторяется пока весь массив не станет отсортирован.
*  **Объяснение работы скрипта:**
   * Внешний цикл повторяет проход по массиву n раз.
   * Внутренний цикл сравнивает пару соседних элементов и меняет их местами, если порядок неправильный.
   * После каждого прохода самый элемент "высовывается" в конец массива, его можно не рассматривать в следующих итерациях.
   * Если за проход не было ни одного обмена (swapped равно False), значит массив уже отсортирован, и можно завершить работу алгоритма.
*  **Временная сложность:**
   * В худшем и среднем случае: O(n²)
   * В лучшем случае (если массив уже отсортирован): O(n), при условии, что мы можем выйти из алгоритма, если на проходе не произошло ни одного обмена.
*  **Объяснение:** В худшем случае, для сортировки массива из n элементов, необходимо пройти по нему n раз, выполняя сравнения и возможные обмены Их общее количество сравнения — примерно n*(n-1)/2, что асимптотически равно O(n²).

## 3) Сортировка вставками- это алгоритм сортировки, который работает "по частям", строя отсортированную часть массива по мере продвижения. Он берет каждый элемент и вставляет его на правильную позицию в уже отсортированной части массива (слева).
Этот алгоритм похож на то, как мы сортируем карточки в руке — вставляя каждую новую карточку на нужное место.
*  **Объяснение работы скрипта:**
   * Начинаем со второго элемента (индекс 1), так как первый считается уже "отсортированным".
   * Берем текущий элемент (key) и ищем правильное место для вставки в отсортированную часть слева.
   * Перещелкиваем все элементы, которые больше key, вправо.
   * Вставляем key на освободившееся место.
   * Повторяем для каждого следующего элемента.
*  **Временная сложность:**
   * В среднем и в худшем случае: O(n²)
   * В лучшем случае (когда массив уже отсортирован): O(n), так как не потребуется перестановок
*  **Объяснение:** В худшем случае, для каждого элемента массива нужно пройтись по всем уже отсортированным элементам (внутренний цикл).
Общее число сравнений и перестановок — пропорционально n*(n-1)/2, что равно O(n²).

## 4) Сортировка слиянием — это эффективный алгоритм "разделяй и властвуй". Он разбивает массив на две части, сортирует каждую из них рекурсивно, а затем объединяет (сливает) отсортированные части в один отсортированный массив. Этот метод работает особенно хорошо для больших данных и гарантирует стабильность сортировки.
*  **Объяснение работы скрипта:**
   * Функция merge_sort вызывает сама себя рекурсивно, разбивая массив на две половины, пока не достигнет базового случая — массив из одного элемента.
   * После того, как меньшие массивы отсортированы (база — один элемент считается отсортированным), происходит объединение двух отсортированных половин в один отсортированный массив.
   * В процессе слияния сравниваются элементы из обеих половин и по одному вставляются в результирующий массив.
   * После окончания слияния массив полностью отсортирован.
* **Временная сложность:**  во всех случаях: O(n log n)
* **Объяснение:** Благодаря тому, что массив постоянно делится пополам, и слияние — линейная операция, общая сложность остается логарифмической по глубине рекурсии и линейной — по времени слияния. Каждый уровень деления массива — это логарифм по основанию 2, так как массив постоянно разбивается пополам.
На каждом уровне требуется пройти по всему массиву для слияния: это O(n).

## 5) Сортировка Шелла-это улучшенная версия сортировки вставками. Она работает за счет того, что элементы сравниваются и сортируются не только соседними, а с промежутками, уменьшающимися по мере выполнения алгоритма. Это помогает «переносить» элементы ближе к их правильным позициям быстрее, что повышает эффективность по сравнению с прямой сортировкой вставками.
   * Принцип:
Исходный массив разбивается на части с интервалом (gap).
Каждая из этих частей сортируется по методу вставок.
Интервал уменьшается и процедура повторяется, пока интервал не станет равен 1 — то есть, массив полностью сортируется.
*  **Объяснение работы скрипта:**
   * Изначально выбирается большой интервал (gap), равный примерно половине длины массива.
   * Внутренний цикл сортирует подсекции массива, расположенные через gap, методом вставки.
   * После прохода с большим интервалом gap он уменьшается вдвое.
   * Когда интервал становится равен 1, массив сортируется стандартной вставкой, но большинство элементов уже на своих местах, что ускоряет итоговую сортировку.
   * В конце массив полностью отсортирован.
*  **Временная сложность:** Зависит от выбранной последовательности gaps. Для последовательности, используемой в коде (n/2^k), сложность составляет O(n^(3/2)) или O(n log² n) в среднем случае, но может быть хуже.
   * В худшем случае (для некоторых последовательностей) может быть O(n²).
   * Для последовательности Кнута (gap = 3h + 1) сложность составляет O(n^(3/2)).
   * В целом, точная сложность часто выражается как O(n^p), где 1 < p <= 2.
* **Объяснение:** Алгоритм использует вложенные циклы, но внутренний цикл (по j) не всегда проходит n раз, как в пузырьке. Количество итераций зависит от gap. В среднем случае количество итераций внутреннего цикла растет медленнее, чем n, приводя к сложности между O(n) и O(n^2).

## 6) Быстрая сортировка- один из самых популярных и быстрых алгоритмов сортировки, использующий стратегию «разделяй и властвуй». Он выбирает опорный элемент, делит массив на две части (элементы меньше и большие этого опорного), рекурсивно сортирует эти части, а затем соединяет результат.
*  **Объяснение работы скрипта:**
   * Если массив из 0 или 1 элемента — он уже отсортирован, возвращаем его.
   * Выбираем опорный элемент (здесь — средний по индексу).
   * Создаем три массива:
     * left: все элементы меньше pivot.
     * middle: все элементы равные pivot.
     * right: все элементы больше pivot.
   * Рекурсивно сортируем left и right.
   * В итоговой строке соединяем отсортированную левую часть, средние элементы и отсортированную правую часть, получая полный отсортированный массив.
*  **Временная сложность:**
   *  В среднем и в худшем случае: O(n log n)
   *  В худшем случае:O(n^2)
*  **Объяснение:** Массив при каждом разбиении делится примерно на две равные части. Поэтому глубина рекурсии — log n.
На каждом уровне обработки требуется пройти все элемента (разделение), что дает O(n)
Итоговая сложность — произведение: O(n) * O(log n) = O(n log n).

## 7) Пирамидальная сортировка (или куча) — это эффективный алгоритм сортировки, который использует структуру данных «куча» (куча). 
*  **Он состоит из двух этапов:** 
   * Построение кучи из исходного массива.
   * Удаление корня (максимального или минимального элемента) кучи и вставка его в отсортированный массив, повторяя процесс для элементов.
   * Преимущество — она работает в основном за O(n log n) независимо от исходных данных.
*  **Объяснение работы скрипта:**
   * Построение кучи: начиная с последнего неполного уровня, превращаем массив в max-heap (кучу с максимальным корнем).
   * Извлечение максимального элемента: меняем его с последним элементом массива, уменьшаем размер кучи и восстанавливаем свойства кучи (heapify) в корне.
   * Повторяем, пока не отсортируем весь массив.
*  **Временная сложность:**
   * В худшем, среднем и лучшем случае: O(n log n)
*  **Объяснение:** Построение кучи — O(n)
Каждая операция удаления максимума (обмен иheapify) — O(log n).
Выполняется таких операций n следовательно — итоговая сложность: O(n log n).

## 8) Последовательный поиск — это самый простой способ найти элемент в массиве (или списке). Он заключается в переборе элементов по порядку, начиная с первого, до тех пор, пока не найдется нужный элемент или не закончится список. Этот метод особенно подходит, если массив не отсортирован или его размер небольшой.
*  **Объяснение работы скрипта:**
   * Проходим по каждому элементу массива с помощью цикла for.
   * На каждой итерации мы сравниваем текущий элемент с искомым (target).
   * If совпало — возвращаем индекс этого элемента.
   * Если цикл завершился без поиска — возвращаем -1, что означает, что элемент не найден.
*  **Временная сложность:**
   * В худшем и среднем случае: O(n), где n — количество элементов.
   * В лучшем случае, если искомый элемент — первый элемент, сложность — O(1).
* **Объяснение:** В худшем случае элемент может находиться в самом конце массива или вообще отсутствовать, поэтому придется проверить все элементы.
Количество проверок напрямую пропорционально размеру массива, поэтому — линейная сложность O(n)

## 9) Бинарный (двоичный, дихотомический) поиск — это поиск заданного элемента на упорядоченном множестве, осуществляемый путём неоднократного деления этого множества на две части таким образом, что искомый элемент попадает в одну из этих частей. Поиск заканчивается при совпадении искомого элемента с элементом, который является границей между частями множества или при отсутствии искомого элемента. Преимуществом бинарного поиска является более низкая трудоёмкость по сравнению с последовательным поиском. Недостаток заключается в том, что он применим только на отсортированных множествах. Алгоритм работает только с отсортированным массивом. На каждом шаге область поиска уменьшается вдвое за счет сравнения с элементом в середине.
*  **Объяснение работы скрипта:**
   * Изначально устанавливаем границы поиска: left и right.
   * Пока границы не пересекаются (left <= right), выполняем цикл:
   * Находим средний индекс mid.
   * Сравниваем значение arr[mid] с искомым.
   * Если равно, возвращаем индекс.
   * Если меньше — сдвигаем левую границу вправо (left = mid + 1).
   * Если больше — сдвигаем правую границу влево (right = mid - 1).
   * Если границы пересеклись и элемент так и не найден, возвращаем -1.
*  **Временная сложность:** 
   * В худшем случае, при каждом шаге массив делится пополам, и в итоге за несколько шагов мы получим искомый элемент или исключим его существование.
   * Время выполнения: O(log n), где n — количество элементов в массиве.
*  **Объяснение:** Потому что на каждом шаге алгоритм делит массив пополам, сокращая пространство поиска вдвое. После k итераций остается проверять только один элемент. Количество таких итераций — это логарифм от размера массива, то есть log₂ n.

## 10) Интерполирующий поиск — это алгоритм поиска для отсортированных наборов данных, таких как массивы или списки. Он предсказывает позицию нужного элемента на основе разницы значений. Эффективен, если элементы распределены достаточно равномерно. Он использует формулу интерполяции, чтобы предположить, где может находиться искомый элемент, основываясь на его предполагаемом положении относительно границ массива. В отличие от бинарного поиска, который делит массив пополам независимо от значений, интерполяционный ищет более "точечно", делая предположения, где может находиться элемент.
*  **Объяснение работы скрипта:** 
   * Задаем начальные границы поиска low и high.
   * Пока границы не пересекаются, делаем предположение о позиции pos с помощью формулы интерполяции: [ pos = low + \frac{(target - arr[low]) \times (high - low)}{arr[high] - arr[low]} ]
   * Проверяем, находится ли предполагаемый индекс pos внутри границ. Если нет — возвращаем -1.
   * Сравниваем элемент arr[pos] с искомым:
   * Если равно — возвращаем индекс.
   * Если меньше — сдвигаем нижнюю границу вверх.
   * Если больше — сдвигаем верхнюю границу вниз.
   * Если элемент не найден — возвращаем -1.
*  **Временная сложность:**
   * В лучшем случае: O(log log n) — когда массив равномерно распределен и поиск максимально эффективен.
   * В худшем случае: O(n) — например, если массив неравномерно распределен или элементы расположены неравномерно, и предположения о позиции оказываются неверными.
*  **Объяснение:**
В идеальных условиях, когда массив равномерно распределён, формулы очень точно предсказывают позицию элемента, и поиск сужается очень быстро. В этом случае время — O(log log n).
В худшем случае, когда распределение неравномерное или предположения о позиции неверны, алгоритм может свести к линейному поиску, то есть O(n).

## 11) Поиск по Фибоначчи — это эффективный алгоритм поиска, используемый для нахождения целевого значения в отсортированной коллекции, такой как массив или список. По принципу он аналогичен бинарному поиску, но использует числа Фибоначчи для определения позиций для сравнения. Этот алгоритм является вариацией бинарного поиска, при которой деление массива происходит по длинам, соответствующим числам Фибоначчи. Он особенно полезен, когда разделение происходит в интервалах с неравномерными размерами.
*  **Объяснение работы скрипта:**
   * Создаем последовательность Фибоначчи, пока число не превысит длину массива.
   * Используем три переменные, чтобы хранить последние два числа Фибоначчи и текущее.
   * В цикле сравниваем элемент, находящийся по расчетному индексу i, с искомым.
   * В зависимости от результата сдвигаем границы поиска, уменьшая или увеличивая диапазон, используя числа Фибоначчи.
   * После цикла проверяем последний возможный индекс, если он соответствует искомому.
   * Возвращаем индекс найденного элемента или -1, если элемент отсутствует.
*  **Временная сложность:**
   * Время поиска: O(log n), аналогично бинарному поиску, так как алгоритм делит массив примерно пополам за счёт чисел Фибоначчи.
*  **Объяснение:** Поскольку алгоритм делит диапазон поиска по числам, связанным с последовательностью Фибоначчи, его эффективность примерно совпадает с бинарным поиском, поэтому сложность — O(log n).
