## 1) Сортировка выбором (Selection Sort) — это простой алгоритм сортировки, который на каждом шаге ищет минимальный элемент в неотсортированной части массива и меняет его местами с первым элементом этой части. Этот процесс повторяется подряд, пока весь массив не станет отсортирован.
*  **Объяснение работы скрипта:**
   * В функции selectionSort есть два вложенных цикла.
   * Внешний цикл перебирает позиции массива от 0 до n-2, повторяя процесс сортировки для каждой позиции.
   * Внутренний цикл ищет минимальный элемент в оставшейся части массива, начиная с позиции i + 1
   * Каждая итерация внутреннего цикла сравнивает текущий элемент arr[j] с текущим минимумом arr[min_idx].
   * Если найден меньший элемент, то переменная min_idx обновляется.
   * После завершения внутреннего цикла, минимальный элемент в диапазоне перемещается на текущую позицию i при помощи операции обмена std::swap.
   * Эта процедура повторяется для каждой позиции, пока весь массив не станет отсортированным.
*  **Временная сложность:**
   * В худшем, среднем и лучшем случаях: O(n²)
   * Алгоритм не особо эффективен для больших данных, так как количество сравнений квадратично растет с ростом массива.
*  **Объяснение:** Для каждого из n элементов нужно найти минимальный среди оставшихся, что требует n, n-1, ..., сравнений.
Общее число сравнений — сумма арифметической прогрессии, которая пропорциональна n*(n-1)/2, что асимптотически равно O(n²).

---

## 2) Сортивка обменом- это простой алгоритм сортировки. Он многократно проходит по массиву, сравнивает соседние элементы и меняет их местами, если они идут в неправильном порядке. Этот процесс повторяется пока весь массив не станет отсортирован.
*  **Объяснение работы скрипта:**
   * В функции bubble_sort есть два вложенных цикла.
   * Внешний цикл повторяет проход по всему массиву n раз.
   * Внутренний цикл сравнивает каждую пару соседних элементов arr[j] и arr[j + 1].
   * Если текущий элемент больше следующего, то они меняют местами при помощи операции обмена arr[j], arr[j + 1] = arr[j + 1], arr[j]
   * После каждого полного прохода самый большой элемент "всплывает" в конец массива, и последующие проходы не рассматривают уже отсортированную часть.
   * Этот процесс повторяется n раз или пока массив не отсортируется.
*  **Временная сложность:**
   * В худшем и среднем случае: O(n²)
   * В лучшем случае (если массив уже отсортирован): O(n), при условии, что мы можем выйти из алгоритма, если на проходе не произошло ни одного обмена.
*  **Объяснение:** В худшем случае, для сортировки массива из n элементов, необходимо пройти по нему n раз, выполняя сравнения и возможные обмены Их общее количество сравнения — примерно n*(n-1)/2, что асимптотически равно O(n²).

---

## 3) Сортировка вставками- это алгоритм сортировки, который работает "по частям", строя отсортированную часть массива по мере продвижения. Он берет каждый элемент и вставляет его на правильную позицию в уже отсортированной части массива (слева).
Этот алгоритм похож на то, как мы сортируем карточки в руке — вставляя каждую новую карточку на нужное место.
*  **Объяснение работы скрипта:**
   * В функции insertion_sort есть внешний цикл, который перебирает каждый элемент массива начиная со второго (i от 1 до len(arr)-1).
   * Для каждого элемента arr[i] — это “ключ” — алгоритм сравнивает его с предыдущими элементами (arr[j]), сдвигая все элементы, которые больше key, на одну позицию вправо.
   * Это реализуется с помощью внутреннего цикла while, который продолжается, пока j >= 0 и arr[j] > key.
   * После того как подходящее место найдено, key вставляется в результирующую позицию.
*  **Временная сложность:**
   * В среднем и в худшем случае: O(n²)
   * В лучшем случае (когда массив уже отсортирован): O(n), так как не потребуется перестановок
*  **Объяснение:** В худшем случае, для каждого элемента массива нужно пройтись по всем уже отсортированным элементам (внутренний цикл).
Общее число сравнений и перестановок — пропорционально n*(n-1)/2, что равно O(n²).

---

## 4) Сортировка слиянием — это эффективный алгоритм, он разбивает массив на две части, сортирует каждую из них рекурсивно, а затем объединяет (сливает) отсортированные части в один отсортированный массив. Этот метод работает особенно хорошо для больших данных и гарантирует стабильность сортировки.
*  **Объяснение работы скрипта:**
   * Алгоритм использует рекурсивное деление массива на две части, затем каждую часть сортирует по отдельности, а затем объединяет их обходом функции merge.
   * Деление массива пополам рекурсивно к массивам из одного элемента.
   * Объединение отсортированных подмассивов методом слияния.
   * Разделение массива: Каждая рекурсивная ступень делит массив пополам, создавая две более маленькие задачи. Глубина рекурсии: [ \log_2 n]
   * Объединение (merge):Каждая операция слияния проходит по всему диапазону, для которого она вызывается, то есть по n элементам в сумме на каждом уровне рекурсии. На каждом уровне рекурсии происходит O(n) операций, связанных со слиянием — копированием элементов и сравнением. А поскольку уровней рекурсии всего log_2 n, то: [ \text{Общая сложность} \approx O(n \log n) ]
* **Временная сложность:**  во всех случаях: O(n log n)
* **Объяснение:** На каждом этапе происходит сравнение и вставка элементов для части массива, а в каждом уровне объединения общий объём данных — все элементы массива. Так как количество уровней — логарифмическое, потому что массив делится пополам на каждом уровне, то объединение осуществляется за линейное время, а уровень деления — логарифмический.

---

## 5) Сортировка Шелла-это улучшенная версия сортировки вставками. Она работает за счет того, что элементы сравниваются и сортируются не только соседними, а с промежутками, уменьшающимися по мере выполнения алгоритма. Это помогает «переносить» элементы ближе к их правильным позициям быстрее, что повышает эффективность по сравнению с прямой сортировкой вставками.
   * Принцип:
Исходный массив разбивается на части с интервалом (gap).
Каждая из этих частей сортируется по методу вставок.
Интервал уменьшается и процедура повторяется, пока интервал не станет равен 1 — то есть, массив полностью сортируется.
*  **Объяснение работы скрипта:**
   * Вычисляется начальный интервал gap = n // 2
   * В цикле while интервал уменьшается вдвое (gap //= 2) на каждом шаге, пока он не станет равен 0.
   * Внутри цикла for происходит сортировка вставками по элементам, отстоящим друг от друга на расстоянии gap.
   * Внутренний цикл while осуществляет перестановку элементов с шагом gap, перемещая меньшие элементы влево.
*  **Временная сложность:** Зависит от выбранной последовательности gaps. Для последовательности, используемой в коде (n/2^k), сложность составляет O(n^(3/2)) или O(n log² n) в среднем случае, но может быть хуже.
   * В худшем случае (для некоторых последовательностей) может быть O(n²).
   * Для последовательности Кнута (gap = 3h + 1) сложность составляет O(n^(3/2)).
   * В целом, точная сложность часто выражается как O(n^p), где 1 < p <= 2.
* **Объяснение:** Алгоритм использует вложенные циклы, но внутренний цикл (по j) не всегда проходит n раз, как в пузырьке. Количество итераций зависит от gap. В среднем случае количество итераций внутреннего цикла растет медленнее, чем n, приводя к сложности между O(n) и O(n^2).

---

## 6) Быстрая сортировка- один из самых популярных и быстрых алгоритмов сортировки, использующий стратегию «разделяй и властвуй». Он выбирает опорный элемент, делит массив на две части (элементы меньше и большие этого опорного), рекурсивно сортирует эти части, а затем соединяет результат.
*  **Объяснение работы скрипта:**
   * Алгоритм выбирается опорный элемент (pivot), и массив разбивается на три части:
   * Все элементы меньше pivot (left),
   * Все равные pivot (middle),
   * Все больше pivot (right).
   * Затем рекурсивно сортируются left и right, а middle остаётся без изменений (так как они уже отсортированы).
   * В реализации используют генераторы списков для разбиения массива, что позволяет пополнить стек вызовов рекурсии.
   * Время выполнения зависит от выбора pivot и структуры массива: В лучшем случае pivot равномерно делит массив, что приводит к сбалансированным вызовам.
*  **Временная сложность:**
   *  В среднем и в худшем случае: O(n log n)
   *  В худшем случае:O(n^2)
*  **Объяснение:** Массив при каждом разбиении делится примерно на две равные части. Поэтому глубина рекурсии — log n.
На каждом уровне обработки требуется пройти все элемента (разделение), что дает O(n)
Итоговая сложность — произведение: O(n) * O(log n) = O(n log n).

---

## 7) Пирамидальная сортировка (или куча) — это эффективный алгоритм сортировки, который использует структуру данных «куча» (куча). 
*  **Он состоит из двух этапов:** 
   * Построение кучи:
   * В цикле от (n / 2 - 1) до 0 вызывается функция heapify.
   * Это создает Max-Heap — структуру данных, в которой родитель всегда больше своих потомков.
   * В цикле от n-1 до 0:
   * Переносим текущий корень (максимальный элемент) в конец массива (swap).
   * Уменьшаем размер кучи на 1.
   * Восстанавливаем свойства кучи вызывая heapify для корня, чтобы восстановить структуру Max-Heap.
   * На каждом из n шагов извлечения, вызывается heapify.
*  **Временная сложность:**
   * В худшем, среднем и лучшем случае: O(n log n)
*  **Объяснение:** Построение кучи — O(n)
Построение пирамиды — heapify вызывает рекурсию с глубиной log n, выполняется быстрыми способами, потому сумма всех рекурсивных вызовов — O(n). Следующий этап — извлечение максимального элемента и перестройка — выполняется n раз, каждый раз heapify — O(log n).

---

## 8) Последовательный поиск — это самый простой способ найти элемент в массиве (или списке). Он заключается в переборе элементов по порядку, начиная с первого, до тех пор, пока не найдется нужный элемент или не закончится список. Этот метод особенно подходит, если массив не отсортирован или его размер небольшой.
*  **Объяснение работы скрипта:**
   * Перебираются все элементы массива arr по порядку с помощью enumerate.
   * На каждом шаге сравнивается текущий элемент element с искомым значением target.
   * Если element равен target, функция возвращает индекс этого элемента (index).
   * Если перебрали весь массив и не нашли искомого элемента, возвращается -1.
*  **Временная сложность:**
   * В худшем и среднем случае: O(n), где n — количество элементов.
   * В лучшем случае, если искомый элемент — первый элемент, сложность — O(1).
* **Объяснение:** В худшем случае: Искомый элемент отсутствует в массиве или находится в самом конце. Тогда перебираются все n элементов. Время: O(n).
В лучшем случае: Искомый элемент находится в первом элементе массива (максимум за 1 сравнение). Время: O(1).
В среднем случае: При равномерном распределении искомого элемента по массиву его позиция может находиться в любой точке, и в среднем потребуется проверка примерно половины элементов. Время: O(n).

---

## 9) Бинарный (двоичный, дихотомический) поиск — это поиск заданного элемента на упорядоченном множестве, осуществляемый путём неоднократного деления этого множества на две части таким образом, что искомый элемент попадает в одну из этих частей. Поиск заканчивается при совпадении искомого элемента с элементом, который является границей между частями множества или при отсутствии искомого элемента. Преимуществом бинарного поиска является более низкая трудоёмкость по сравнению с последовательным поиском. Недостаток заключается в том, что он применим только на отсортированных множествах. Алгоритм работает только с отсортированным массивом. На каждом шаге область поиска уменьшается вдвое за счет сравнения с элементом в середине.
*  **Объяснение работы скрипта:**
   * В алгоритме поддерживаются два указателя: left и right, обозначающие границы текущего подмассива.
   * В каждом шаге вычисляется mid — индекс среднего элемента.
   * Если arr[mid] равен искомому target, поиск завершён успешно, возвращается индекс.
   * Если arr[mid] меньше target, поиск продолжается в правой половине массива, и left смещается после mid.
   * Если arr[mid] больше target, поиск продолжается в левой половине, и right смещается перед mid.
   * Цикл продолжается, пока left не превысит right.
*  **Временная сложность:** 
   * В худшем случае, при каждом шаге массив делится пополам, и в итоге за несколько шагов мы получим искомый элемент или исключим его существование.
   * Время выполнения: O(log n), где n — количество элементов в массиве.
*  **Объяснение:** В худшем случае: Каждый шаг делит текущий диапазон пополам, и в худшем случае элемент находится в самом конце или не в массиве вообще. Количество итераций — [ \log_2 n ] — потому что каждый раз диапазон уменьшается вдвое.
В лучшем случае: Элемент находится в середине массива сразу — поиск завершится за 1 сравнение. Время: O(1).
В среднем случае: При случайных данных и равномерном распределении элементов — также O(log n).

---

## 10) Интерполирующий поиск — это алгоритм поиска для отсортированных наборов данных, таких как массивы или списки. Он предсказывает позицию нужного элемента на основе разницы значений. Эффективен, если элементы распределены достаточно равномерно. Он использует формулу интерполяции, чтобы предположить, где может находиться искомый элемент, основываясь на его предполагаемом положении относительно границ массива. В отличие от бинарного поиска, который делит массив пополам независимо от значений, интерполяционный ищет более "точечно", делая предположения, где может находиться элемент.
*  **Объяснение работы скрипта:** 
   * Алгоритм похож на бинарный поиск, но вместо деления массива пополам, вычисляет предполагаемую позицию pos по формуле интерполяции, основываясь на предполагаемом равномерном распределении элементов.
   * Задача — примерно предсказать, где находится искомый элемент, чтобы искать быстрее.
   * В цикле проверяем, что target находится в пределах текущего диапазона (arr[low] и arr[high]) и что границы допустимы.
   * Используем формулу, чтобы вычислить pos, исходя из ожидаемого расположения target.
   * После этого сравниваем arr[pos] с target и корректируем границы (low, high) в зависимости от результата.
*  **Временная сложность:**
   * В лучшем случае: O(log log n) — когда массив равномерно распределен и поиск максимально эффективен.
   * В худшем случае: O(n) — например, если массив неравномерно распределен или элементы расположены неравномерно, и предположения о позиции оказываются неверными.
*  **Объяснение:** В худшем случае: Если распределение элементов неравномерное или набор данных очень сложен, график работы может ухудшиться. В худшем случае интерполяционный поиск деградирует до линейного поиска — O(n), потому что в неподходящих случаях могут возникать очень плохие предсказания, приводящие к худшей эффективности.
В лучшем и среднем случаях: При равномерном распределении элементов, прогнозируемое положение быстро сужается к искомому элементу, и поиск выполняется за O(log log n). Это достигается, потому что интерполяционный поиск, в среднем, работает быстрее бинарного при равномерном распределении.

---

## 11) Поиск по Фибоначчи — это эффективный алгоритм поиска, используемый для нахождения целевого значения в отсортированной коллекции, такой как массив или список. По принципу он аналогичен бинарному поиску, но использует числа Фибоначчи для определения позиций для сравнения. Этот алгоритм является вариацией бинарного поиска, при которой деление массива происходит по длинам, соответствующим числам Фибоначчи. Он особенно полезен, когда разделение происходит в интервалах с неравномерными размерами.
*  **Объяснение работы скрипта:**
   * Алгоритм использует числа Фибоначчи для разделения массива на части.
   * Сначала вычисляется минимальное число Фибоначчи (F[m]), такое что F[m] >= n, где n — длина массива.
   * Обозначим fib_m, fib_m1, fib_m2 — числа Фибоначчи, соответствующие текущему шагу, и используемые для индексации.
   * В цикле сравнивается элемент arr[i], где i = min(offset + fib_m2, n-1):
   * Если arr[i] < x, то сдвигаемся вправо, обновляя fib_m, fib_m1, fib_m2 так, чтобы "скользить" на меньшие числа Фибоначчи.
   * Если arr[i] > x, то сдвигаемся влево, обновляя соответствующие числа Фибоначчи.
   * Если найден элемент — возвращается его индекс.
   * Во время поиска массив фактически разделяется на части, размеры которых связаны с числами Фибоначчи, что обеспечивает схему поиска.
*  **Временная сложность:**
   * Время поиска: O(log n), аналогично бинарному поиску, так как алгоритм делит массив примерно пополам за счёт чисел Фибоначчи.
*  **Объяснение:** Каждый шаг уменьшает диапазон поиска экспоненциально, а логарифм отражает, сколько раз нужно разбить число на части, чтобы получить один элемент. Чем больше массив — тем больше нужно шагов.

---

